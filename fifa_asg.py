# -*- coding: utf-8 -*-
"""FIFA Asg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aQz2lQ_Ig5U0MbFczXp0ISa2VmMK7ZAt
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from google.colab import drive
import pickle as pkl

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
drive.mount('/content/drive')

!pip install streamlit
import streamlit as st

FIFA_data=pd.read_csv('/content/drive/MyDrive/players_22 (1).csv')
# FIFA_data.head(10)

def data_preprocess(data):
  ''' A method that removes useless varaiables, performs EDA, imputation and encoding.'''

  # Drop columns with more than 30% missing data
  threshold = int(0.3 * len(data))
  data.dropna(thresh=threshold, axis=1,inplace=True)

  # Keep dropping irrelevant columns
  data.drop(['ls', 'st', 'rs',
       'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm',
       'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb',
       'rcb', 'rb', 'gk'], axis=1,inplace=True)
  # Keep dropping irrelevant columns based on name
  data = data[data.columns.drop(list(data.filter(regex='id|url|name|age|face|work|club|dob|eur|foot|reputation|height|weight|body|nation|player|power|position|physic')))]

  # Impute Resultant Data
  imp = IterativeImputer(max_iter=10, random_state=0)
  data = pd.DataFrame(np.round(imp.fit_transform(data)), columns=data.columns)
  return data

FIFA_data_processed = data_preprocess(FIFA_data.copy())

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.preprocessing import StandardScaler

def feature_engineering(data):
  '''A function that creates feature subsets which show better correlation with the overall rating and scaled the independent variables.'''

  # Select the target variable
  Y = data['overall']

  # Select the predictor variables and selet the best 10
  X = data.drop('overall',axis=1)
  selector = SelectKBest(f_classif, k=10)
  X_selected = selector.fit_transform(X,Y)
  # Get the boolean mask of selected features

  selected_features = X.columns[selector.get_support()]

  # Create a DataFrame with the selected features
  X_selected_df = pd.DataFrame(X_selected, columns=selected_features)

  selected_features_dict = {}

  # Populate the dictionary with feature names, max, min, and sample values
  for column in X_selected_df.columns:
    max_value = X_selected_df[column].max()
    min_value = X_selected_df[column].min()
    sample_value = X_selected_df[column].mean()
    selected_features_dict[column] = {
        'max_value': max_value,
        'min_value': min_value,
        'sample_value': sample_value
      }

  # Write the dictionary to a pickle file
  with open('selected_features_dict.pkl', 'wb') as file:
    pkl.dump(selected_features_dict, file)

  # scale the predictor variables data
  scaler = StandardScaler()
  scaled_data = scaler.fit_transform(X_selected)

  return (scaled_data, Y)

!pip install xgboost

new = feature_engineering(FIFA_data_processed)
print(new)

from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

from sklearn.metrics import mean_absolute_error, mean_squared_error

def train_model(variables):
  '''A method that creates and trains data with cross-validation (RandomForest, XGBoost, Gradient Boost Regressors)  that can predict a player rating.'''

  # Split data into train and test data
  Xtrain,Xtest,Ytrain,Ytest=train_test_split(variables[0],variables[1],test_size=0.2,random_state=42)

  rfr = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42)
  xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
  gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42 )
  for model in (rfr, xgb, gbr):
    model.fit(Xtrain, Ytrain)
    pkl.dump(model, open('/content/' + model.__class__.__name__ + '.pkl', 'wb'))
    y_pred = model.predict(Xtest)
    print(f'''{model.__class__.__name__},
    Mean Absolute Error (MAE):  {mean_absolute_error(y_pred, Ytest)},
    Root Mean Squared Error (RMSE): {np.sqrt(mean_squared_error(y_pred, Ytest))}
    ''')
    return rfr, xgb, gbr,  Xtrain,Xtest,Ytrain,Ytest

train = train_model(new)
print(train)

from sklearn.model_selection import GridSearchCV

def evaluation(model1, model2, model3, trainX, testX, trainY, testY):
  # Define hyperparameter grids for each model
  param_grid1 = {
      'n_estimators': [50, 100, 200],
      'max_depth': [None, 5, 10],
      'min_samples_split': [2, 5, 7]
  }

  param_grid2 = {
      'n_estimators': [50, 100, 200],
      'learning_rate': [0.01, 0.1, 0.2],
      'max_depth': [3, 5, 7]
  }

  param_grid3 = {
      'n_estimators': [50, 100, 200],
      'learning_rate': [0.01, 0.1, 0.2],
      'max_depth': [3, 5, 7]
  }

  # Create a dictionary to store the models and their parameter grids
  models = {      model1: param_grid1,
      model2: param_grid2,
      model3: param_grid3
  }

  # Loop through the models, perform grid search, and evaluate
  for model, param_grid in models.items():
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_absolute_error')
    grid_search.fit(trainX, trainY)

    best_model = grid_search.best_estimator_
    pkl.dump(best_model, open('/content/' + best_model.__class__.__name__ + '_tuned.pkl', 'wb'))

    y_pred = best_model.predict(testX)
    print(f'''{best_model.__class__.__name__},
    Mean Absolute Error (MAE):  {mean_absolute_error(y_pred, testY)},
    Root Mean Squared Error (RMSE): {np.sqrt(mean_squared_error(y_pred, testY))}
    ''')
    print("Best Hyperparameters:", grid_search.best_params_)
    print("-" * 30)

eval = evaluation(train[0],train[1],train[2],train[3],train[4],train[5],train[6])

"""# Testing Data"""

!pip install joblib

import joblib

# Load your trained model (assuming you saved it after training)
model = joblib.load('best_model.pkl')

# Load your new data (assuming it's in a CSV file or any other format)
players_22 = pd.read_csv('/content/drive/MyDrive/players_22 (1).csv')
players_processed = data_preprocess(players_22)
player_feature = feature_engineering(players_processed)


X_new = player_feature[0]

actual_value = player_feature[1]

# Make predictions on the new data
y_pred_new = model.predict(X_new)

# Compare predictions with actual data
mae = mean_absolute_error(actual_value, y_pred_new)
rmse = np.sqrt(mean_squared_error(actual_value, y_pred_new))

print(f'''\n\nMean Absolute Error (MAE): {mae},
Root Mean Squared Error (RMSE): {rmse}
''')

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Actual Values': actual_value,
    'Predicted Values': y_pred_new
})

print(f'''Prediction Comparison DataFrame:"
      {comparison_df}
''')

"""# Deploy Trained Model"""